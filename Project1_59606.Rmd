---
title: "Project1_59606"
output: html_document
---

##Data pre-processing 

With infection as the target variable, choose and adjust an adequate
Bayesian regression model (linear, generalized linear or logistic) on a training
subset of the whole data, using a Monte Carlo Markov Chain procedure. 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Loading all libraries that will be required.

```{r}
library(tidyverse)
library(R2jags)
library(coda)
library(bridgesampling)
library(lattice)
library(ggmcmc)

```


Load dataset and examine it.

```{r}
infection <- read_csv("infections.csv"); infection
```

The dataset contains several variables categorical (sex in column 3, prevAB- previous antibiotics (Yes / No) in column 5) and numerical variables (fewer_hours in column 1, wcc - white cell count in column 4, pct - procalcitonin levels in column 6, crp - c-reactive protein in column 7) and a categorical target variable - infection. 

First we set our changes our character variables to binary variables. 
```{r}
infection$infection<-ifelse(as.character(infection$infection) == "Yes", 1, 0)
infection$sex<-ifelse(as.character(infection$sex) == "F", 1, 0)
infection$prevAB<-ifelse(as.character(infection$prevAB) == "Yes", 1, 0)
infection
```


We change our character variables to factors and normalise the rest of the variables.

```{r}
for (i in c(3, 5)) {
  infection[[i]] <- as_factor(infection[[i]])
}

for (i in c(1, 2, 4, 6, 7)) {
  infection[[i]] <- scale(infection[[i]])
}
infection
```


In the project we were asked to split our dataset into tarining and test datasets. 
```{r}
infection<-as.data.frame(infection)
infection
```

Divide dataset into training and test datasets. I have decided to slipt dataset to 70/30 training/test. 


```{r}
split_dummy <- sample(c(rep(0, 0.7 * nrow(infection)),  # Create dummy for splitting
                        rep(1, 0.3 * nrow(infection))))
```

```{r}
infection_train <- infection[split_dummy == 0, ] # Create train data
infection_test <- infection[split_dummy == 1, ] # Create test data
infection_train
```



```{r}
summary(infection_train)

```


```{r}

#write.csv(infection_train, "infections1.csv")
```



##Model adjustment with training dataset

Model matrix, with all variables - this is my null hypothesis.

```{r}


X <- model.matrix(infection ~ ., data = infection_train)
n <- nrow(X);n
k <- ncol(X);k

```




Model specification in JAGS dialect using logistic regression. 

```{r}
model_code <- "
model
{
  # Likelihood
  for (i in 1:n) {
    y[i] ~ dbern(max(0, min(eta[i], 1)))
    logit(eta[i]) <- inprod(beta[], X[i, ])
  }

  # Priors
  for (j in 1:k) {
      beta[j] ~ dnorm(0, 1.2^-2)
  }
}
"
```



```{r}
model_data <- list(n = n, k = k, y = infection_train$infection, X = X)
model_parameters <- c("beta")
```


The following call jags reads, compiles, runs and returns the model information along with MCMC samples and summary statistics.

```{r}
model_run <- jags(
  data = model_data,
  parameters.to.save = model_parameters,
  model.file = textConnection(model_code),
  n.chains = 2,
  n.iter = 8000,
  n.burnin = 100,
  n.thin = 20
)


```



## Check for the convergence of the generated chains. 


```{r}
print(model_run)
```

The Deviance information criterion (DIC) is a commonly applied method to summarize the fit of an MCMC chain and to access model correctness. When comparing different models, lower DIC has more evidence that this is the correct model.



We can get further summary statistics for the posterior.
```{r}
model_run_mcmc <- as.mcmc(model_run)
```


```{r}
summary(model_run_mcmc)
```
The results give the posterior means, posterior standard deviations,
and posterior quantiles for each variable.The “naive” standard error is the standard error of the mean, which captures simulation error of the mean rather than posterior
uncertainty. The time-series standard error adjusts the “naive” standard error for
autocorrelation. During first model runs with less iterations, time-series standard error was at least 30 times larger than SD of posterior distribution for that parameter, indicating computational MCMC error that was fixed after I increased iterations. 





Diagnostic trace plots and density plots shown below.

A traceplots are important for assessing convergence and diagnosing chain
problems. We expect for a good within-chain and between-chain traceplots (red and black colors in the plots) to look random. Indeed, our traceplots do not indicate any convergence problems.

We can also see posterior density plots, however they are not exactly ued for convergence diagnostics, but just to see shape of posterior. 


```{r}
plot(model_run_mcmc)
```

Another good way to look at convergence issues is to look at the plot with the running mean of the chains, which shows if each of the chain is slowly or
quickly approaching its target distribution. What we can see in our plot that all beta parameters are approaching mean quickly and lastly deviance. When comparing both chains, they do converge to the same mean.

```{r}
#First convert MCMC run into suitable dataframe for used package.
S <- ggs(model_run_mcmc) #ggs() produces a data frame object with four variables (iteration, chain, parameter,value) from MCMC run.
ggs_running(S)


```


Autocorrelation analysis is not exactly a tool assesing convergence, however it is useful in order to see if there are potential problems in the model, we expect a correlation in the first lag and not in the next lags, which we do not see in our model.


```{r}
autocorr.diag(model_run_mcmc)

```

This can easily be plotted as an explanation. We can see autocorrelation at first lag (shown as a vertical line) and no autocorrelation further. 

```{r}
autocorr.plot(model_run_mcmc)

```

We can also inspect visually potential problems of convergence if there are any highly correlated parameters,with the correlations between all pairs of parameters. Here we can see that beta 1 has weak negative correlation with beta 4 and beta 6. 


```{r}
 ggs_crosscorrelation(S)
```



Geweke z-score diagnostic focuses on the comparison of the two nonoverlapping parts of the Markov chain and compares the means of both parts, using a difference of means test
to see if the two parts of the chain are from the same distribution. The expected outcome is to have 95 percent of the values between −2 and 2. In our model, there are few values that do not fall it this region, but most values seems ok.


```{r}
geweke.plot(model_run_mcmc)
```


Potential scale reduction factor relies on different chains for the same parameter, by comparing the between-chain variation with the within-chain variation. As expected for good convergence in our model it is close to 1. Values below 1.1 are considered to be acceptable.


```{r}
gelman.diag(model_run_mcmc)

```





##Bayesian factor estimation


Determine if the model is suitable by comparing it with a null (with no
covariates) model, using the Bayes factor.

First wrap our null hypothesis.

```{r}
chains <- tibble()
for (i in 1:length(model_run_mcmc)) {
  chains <- bind_rows(chains, as_tibble(model_run_mcmc[[i]]))
}
chains
```



```{r}
ngen <- 320
y_gen <- matrix(NA, ngen, n)
mean_y_gen <- rep(NA, ngen)
mean_col_X <- colMeans(X)
for (igen in 1:ngen) {
  id_gen <- sample(1:nrow(chains), 1)
  beta_sim <- chains[id_gen, 1:dim(X)[2]]
  for (i in 1:n) {
      y_gen[igen, i] <- as.numeric(rbernoulli(1, 1 / (1 + exp(-sum(beta_sim * X[i, ])))))
  }
  mean_y_gen[igen] <- mean(y_gen[igen, ])
}
mean_y_gen <- tibble(`mean Y` = mean_y_gen)
```



```{r}
pm <- ggplot(data = mean_y_gen, aes(x = `mean Y`)) + geom_density(fill = "blue", alpha = 0.3) + geom_vline(xintercept = mean(infection_train$infection), color = "red")
pm
```

We need to define our alternative H1 hypothesis. 


```{r}

X0 <- model.matrix(infection_train$infection ~ 1 , data = infection_train)
n0 <- nrow(X0);n0
k0 <- ncol(X0);k0

```


Model specification in JAGS dialect.

```{r}
model_code0 <- "
model
{
  # Likelihood
  for (i in 1:n) {
    y[i] ~ dbern(max(0, min(eta[i], 1)))
    logit(eta[i]) <- inprod(beta[], X[i, ])
  }

  # Priors
  for (j in 1:k) {
      beta[j] ~ dnorm(0, 1.2^-2)
  }
}
"
```



```{r}
model_data0 <- list(n = n0, k = k0, y = infection_train$infection, X = X0)
model_parameters0 <- c("beta")
```


```{r}
model_run0 <- jags(
  data = model_data0,
  parameters.to.save = model_parameters0,
  model.file = textConnection(model_code0),
  n.chains = 2,
  n.iter = 8000,
  n.burnin = 100,
  n.thin = 20
)

#n.chains = 4,
 # n.iter = 5000,
 # n.burnin = 200,
 # n.thin = 5

```


```{r}
model_run_mcmc0 <- as.mcmc(model_run0)
```

```{r}
summary(model_run_mcmc0)
```




```{r}
chains0 <- tibble()
for (i in 1:length(model_run_mcmc0)) {
  chains0 <- bind_rows(chains0, as_tibble(model_run_mcmc0[[i]]))
}
chains0
```


Bayesian factor estimation 

```{r}
h0prior <- pnorm(0, mean = 0, sd = 1.2)
h0post <- sum(chains$`beta[2]` <= 0) / dim(chains)[1] 

h1prior <- pnorm(0, mean = 0, sd = 1.2)
h1post <- sum(chains0$beta <= 0) / dim(chains0)[1]


bf <- (h1post / h1prior) / (h0post / h0prior); bf



```

Hence, there is a strong evidence for H0 hypothesis (when it is low) (our null hypothesis) against H1 (new alternative) hypothesis.







PART 3




Present a predictive performance measure of the model using the test
subset (remaining observations).




















